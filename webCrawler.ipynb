{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af518ac5-91fe-43e3-866b-43bb0bd3b3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-pip install requests beautifulsoup4 pandas spacy\n",
    "#python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a15b14f-088b-4662-9434-679181eaddc1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extracted for Khan\n",
      "Data extracted for Aalst\n",
      "Data extracted for Aaronson\n",
      "Data extracted for Abebe\n",
      "Data extracted for Abelson\n",
      "Data extracted for Abiteboul\n",
      "Data extracted for Abramsky\n",
      "Data extracted for Adleman\n",
      "Data extracted for Agrawal\n",
      "Data extracted for Ahn\n",
      "Data extracted for Aho\n",
      "Data extracted for Allen\n",
      "Data extracted for Amdahl\n",
      "Data extracted for Anderson\n",
      "Data extracted for Anthony\n",
      "Data extracted for Appel\n",
      "Data extracted for Aragon\n",
      "Data extracted for Arden\n",
      "Data extracted for Jones\n",
      "Data extracted for Arora\n",
      "Data extracted for Asprey\n",
      "Data extracted for Atanasoff\n",
      "Data extracted for Atre\n",
      "Data extracted for Babbage\n",
      "Data extracted for Bachman\n",
      "Data extracted for Backhouse\n",
      "Data extracted for Backus\n",
      "Data extracted for Bacon\n",
      "Data extracted for Bader\n",
      "Data extracted for Bahl\n",
      "Data extracted for Barr\n",
      "Data extracted for Bartik\n",
      "Data extracted for Barto\n",
      "Data extracted for Bauer\n",
      "Data extracted for Bayer\n",
      "Data extracted for Bell\n",
      "Data extracted for Bellovin\n",
      "Data extracted for Berdichevsky\n",
      "Data extracted for Berners-Lee\n",
      "Data extracted for Bernstein\n",
      "Data extracted for Bernus\n",
      "Data extracted for Bhushan\n",
      "Data extracted for Bjørner\n",
      "Data extracted for Blaauw\n",
      "Data extracted for Black\n",
      "Data extracted for Blei\n",
      "Data extracted for Blum\n",
      "Data extracted for Blum\n",
      "Data extracted for Blum\n",
      "Data extracted for Boehm\n",
      "Data extracted for Böhm\n",
      "Data extracted for Bollacker\n",
      "Data extracted for Bonwick\n",
      "Data extracted for Booch\n",
      "Data extracted for Boole\n",
      "Data extracted for Booth\n",
      "Data extracted for Booth\n",
      "Data extracted for Borg\n",
      "Data extracted for Bos\n",
      "Data extracted for Botvinnik\n",
      "Data extracted for Bowen\n",
      "Data extracted for Bourne\n",
      "Data extracted for Bouwman\n",
      "Data extracted for Boyer\n",
      "Data extracted for Brandenburg\n",
      "Data extracted for Brassard\n",
      "Data extracted for Breed\n",
      "Data extracted for Bresenham\n",
      "Data extracted for Brin\n",
      "Data extracted for Brown\n",
      "Data extracted for Hansen\n",
      "Data extracted for Brinkkemper\n",
      "Data extracted for Brooks\n",
      "Data extracted for Brooks\n",
      "Data extracted for Burnett\n",
      "Data extracted for Butler\n",
      "Data extracted for Gil\n",
      "Data extracted for Camp\n",
      "Data extracted for Campbell-Kelly\n",
      "Data extracted for Candlin\n",
      "Data extracted for Canion\n",
      "Data extracted for Cantrill\n",
      "Data extracted for Cardelli\n",
      "Data extracted for Carmack\n",
      "Data extracted for Caspersen\n",
      "Data extracted for Catmull\n",
      "Data extracted for Cerf\n",
      "Data extracted for Chaitin\n",
      "Data extracted for Cailliau\n",
      "Data extracted for Chaochen\n",
      "Data extracted for Chen\n",
      "Data extracted for Chiariglione\n",
      "Data extracted for Chou\n",
      "Data extracted for Church\n",
      "Data extracted for Ciaramella\n",
      "Data extracted for Clarke\n",
      "Data extracted for Cocke\n",
      "Data extracted for Codd\n",
      "Data extracted for Cohen\n",
      "Data extracted for Coldwater\n",
      "Data extracted for Colton\n",
      "Data extracted for Colmerauer\n",
      "Data extracted for Comer\n",
      "Data extracted for Compton\n",
      "Data extracted for Conway\n",
      "Data extracted for Cormack\n",
      "Data extracted for Cook\n",
      "Data extracted for Cooley\n",
      "Data extracted for Cooper\n",
      "Data extracted for Corbató\n",
      "Data extracted for Cosper\n",
      "Data extracted for Cousot\n",
      "Data extracted for Cox\n",
      "Data extracted for Cray\n",
      "Data extracted for Cristianini\n",
      "Data extracted for Crowcroft\n",
      "Data extracted for Croft\n",
      "Data extracted for Culler\n",
      "Data extracted for Curry\n",
      "Data extracted for Dadda\n",
      "Data extracted for Dahl\n",
      "Data extracted for Dahl\n",
      "Data extracted for Dam\n",
      "Data extracted for Das\n",
      "Data extracted for Daswani\n",
      "Data extracted for Date\n",
      "Data extracted for Davis\n",
      "Data extracted for Dean\n",
      "Data extracted for Demaine\n",
      "Data extracted for DeMarco\n",
      "Data extracted for DeMillo\n",
      "Data extracted for Denning\n",
      "Data extracted for Denning\n",
      "Data extracted for Dertouzos\n",
      "Data extracted for Dewdney\n",
      "Data extracted for Dewar\n",
      "Data extracted for Dham\n",
      "Data extracted for Dietz\n",
      "Data extracted for Diffie\n",
      "Data extracted for Dijkstra\n",
      "Data extracted for Dillon\n",
      "Data extracted for Dix\n",
      "Data extracted for Dongarra\n",
      "Data extracted for Dorigo\n",
      "Data extracted for Dourish\n",
      "Data extracted for Draper\n",
      "Data extracted for Dumais\n",
      "Data extracted for Dunkels\n",
      "Data extracted for Dunn\n",
      "Data extracted for Dustdar\n",
      "Data extracted for Eades\n",
      "Data extracted for Easley\n",
      "Data extracted for Ebbinkhuijsen\n",
      "Data extracted for Eckert\n",
      "Data extracted for Edelman\n",
      "Data extracted for Eich\n",
      "Data extracted for Emeagwali\n",
      "Data extracted for Emerson\n",
      "Data extracted for Engelbart\n",
      "Data extracted for Engelhardt\n",
      "Data extracted for Eppstein\n",
      "Data extracted for Yershov\n",
      "Data extracted for Estridge\n",
      "Data extracted for Etzioni\n",
      "Data extracted for Evans\n",
      "Data extracted for Evans\n",
      "Data extracted for Even\n",
      "Data extracted for Fahlman\n",
      "Data extracted for Feigenbaum\n",
      "Data extracted for Felten\n",
      "Data extracted for Finin\n",
      "Data extracted for Finkel\n",
      "Data extracted for Firesmith\n",
      "Data extracted for Flake\n",
      "Data extracted for Flowers\n",
      "Data extracted for Floyd\n",
      "Data extracted for Floyd\n",
      "Data extracted for Fogel\n",
      "Data extracted for Foley\n",
      "Data extracted for Forbus\n",
      "Data extracted for Jr.\n",
      "Data extracted for Fortnow\n",
      "Data extracted for Fayed\n",
      "Data extracted for Fowler\n",
      "Data extracted for France\n",
      "Data extracted for Franke\n",
      "Data extracted for Fredkin\n",
      "Data extracted for Freund\n",
      "Data extracted for Friedman\n",
      "Data extracted for Fischer\n",
      "Data extracted for Fu\n",
      "Data extracted for Fu\n",
      "Data extracted for Fukushima\n",
      "Data extracted for Fulkerson\n",
      "Data extracted for Gabriel\n",
      "Data extracted for Galil\n",
      "Data extracted for Galler\n",
      "Data extracted for García-Molina\n",
      "Data extracted for Garey\n",
      "Data extracted for Garis\n",
      "Data extracted for Gates\n",
      "Data extracted for Gelernter\n",
      "Data extracted for Gelobter\n",
      "Data extracted for Geschke\n",
      "Data extracted for Ghahramani\n",
      "Data extracted for Ghemawat\n",
      "Data extracted for Gibbons\n",
      "Data extracted for Gilbert\n",
      "Data extracted for Giles\n",
      "Data extracted for Ginsburg\n",
      "Data extracted for Glass\n",
      "Data extracted for Gödel\n",
      "Data extracted for Goel\n",
      "Data extracted for Goguen\n",
      "Data extracted for Gohel\n",
      "Data extracted for Gold\n",
      "Data extracted for Goldberg\n",
      "Data extracted for Goldberg\n",
      "Data extracted for Goldberg\n",
      "Data extracted for Goldsmith\n",
      "Data extracted for Goldreich\n",
      "Data extracted for Goldwasser\n",
      "Data extracted for Golub\n",
      "Data extracted for Golumbic\n",
      "Data extracted for Gonnet\n",
      "Data extracted for Goodfellow\n",
      "Data extracted for Gosling\n",
      "Data extracted for Graham\n",
      "Data extracted for Graham\n",
      "Data extracted for Graham\n",
      "Data extracted for Gray\n",
      "Data extracted for Greibach\n",
      "Data extracted for Gries\n",
      "Data extracted for Griswold\n",
      "Data extracted for Gropp\n",
      "Data extracted for Gruber\n",
      "Data extracted for Guberman\n",
      "Data extracted for Guha\n",
      "Data extracted for Gunther\n",
      "Data extracted for Gutknecht\n",
      "Data extracted for Guy\n",
      "Data extracted for Griesemer\n",
      "Data extracted for Habermann\n",
      "Data extracted for Hahn\n",
      "Data extracted for Hall\n",
      "Data extracted for Hall\n",
      "Data extracted for Halpern\n",
      "Data extracted for Hamilton\n",
      "Data extracted for Hamming\n",
      "Data extracted for Han\n",
      "Data extracted for Harary\n",
      "Data extracted for Harris\n",
      "Data extracted for Hartmanis\n",
      "Data extracted for Håstad\n",
      "Data extracted for Hatton\n",
      "Data extracted for Hawryszkiewycz\n",
      "Data extracted for Jifeng\n",
      "Data extracted for Hehner\n",
      "Data extracted for Hellman\n",
      "Data extracted for Heiser\n",
      "Data extracted for Hendler\n",
      "Data extracted for Hennessy\n",
      "Data extracted for Herbert\n",
      "Data extracted for Hewitt\n",
      "Data extracted for Hightower\n",
      "Data extracted for Hillis\n",
      "Data extracted for Hinton\n",
      "Data extracted for Hirschberg\n",
      "Data extracted for Ho\n",
      "Data extracted for Hoare\n",
      "Data extracted for Hodes\n",
      "Data extracted for Holberton\n",
      "Data extracted for Holland\n",
      "Data extracted for Hollerith\n",
      "Data extracted for Holzmann\n",
      "Data extracted for Hopcroft\n",
      "Data extracted for Hopper\n",
      "Data extracted for Horvitz\n",
      "Data extracted for Householder\n",
      "Data extracted for Hudak\n",
      "Data extracted for Huffman\n",
      "Data extracted for Hughes\n",
      "Data extracted for Hui\n",
      "Data extracted for Humphrey\n",
      "Data extracted for Hutchins\n",
      "Data extracted for Ichbiah\n",
      "Data extracted for Ierusalimschy\n",
      "Data extracted for Ingalls\n",
      "Data extracted for Irwin\n",
      "Data extracted for Iverson\n",
      "Data extracted for Jacobson\n",
      "Data extracted for Jain\n",
      "Data extracted for Jain\n",
      "Data extracted for James\n",
      "Data extracted for Johnson\n",
      "Data extracted for Johnson\n",
      "Data extracted for Jones\n",
      "Data extracted for Jordan\n",
      "Data extracted for Joseph\n",
      "Data extracted for Joshi\n",
      "Data extracted for Joy\n",
      "Data extracted for Jurafsky\n",
      "Data extracted for Kahan\n",
      "Data extracted for Kahn\n",
      "Data extracted for Kak\n",
      "Data extracted for Kamp\n",
      "Data extracted for Karger\n",
      "Data extracted for Karp\n",
      "Data extracted for Karmarkar\n",
      "Data extracted for Karpinski\n",
      "Data extracted for Kaehler\n",
      "Data extracted for Kay\n",
      "Data extracted for Kayal\n",
      "Data extracted for Kellis\n",
      "Data extracted for Kemeny\n",
      "Data extracted for Kennedy\n",
      "Data extracted for Kernighan\n",
      "Data extracted for Kesselman\n",
      "Data extracted for Kiczales\n",
      "Data extracted for Kirstein\n",
      "Data extracted for Kleene\n",
      "Data extracted for Klein\n",
      "Data extracted for Kleinrock\n",
      "Data extracted for Knuth\n",
      "Data extracted for Koenig\n",
      "Data extracted for Koller\n",
      "Data extracted for Kölling\n",
      "Data extracted for Kolmogorov\n",
      "Data extracted for Kolodner\n",
      "Data extracted for Korn\n",
      "Data extracted for Koster\n",
      "Data extracted for Kowalski\n",
      "Data extracted for Koza\n",
      "Data extracted for Krogstie\n",
      "Data extracted for Kruskal\n",
      "Data extracted for Kruusmaa\n",
      "Data extracted for Kurtz\n",
      "Data extracted for Ladner\n",
      "Data extracted for Lam\n",
      "Data extracted for Lamport\n",
      "Data extracted for Lampson\n",
      "Data extracted for Landin\n",
      "Data extracted for Lane\n",
      "Data extracted for Langefors\n",
      "Data extracted for Lattner\n",
      "Data extracted for Lawrence\n",
      "Data extracted for Lazowska\n",
      "Data extracted for Lederberg\n",
      "Data extracted for Lehman\n",
      "Data extracted for Leiserson\n",
      "Data extracted for Lenat\n",
      "Data extracted for LeCun\n",
      "Data extracted for Lerdorf\n",
      "Data extracted for Levchin\n",
      "Data extracted for Levin\n",
      "Data extracted for Leyton-Brown\n",
      "Data extracted for Licklider\n",
      "Data extracted for Liddle\n",
      "Data extracted for Liedtke\n",
      "Data extracted for Lions\n",
      "Data extracted for Lindsey\n",
      "Data extracted for Lipton\n",
      "Data extracted for Liskov\n",
      "Data extracted for Liu\n",
      "Data extracted for Long\n",
      "Data extracted for Lopez\n",
      "Data extracted for Lovegrove\n",
      "Failed to extract data for URL https://en.wikipedia.org/wiki/Ada_Lovelace: 'NoneType' object has no attribute 'find_next_sibling'\n",
      "Data extracted for Luckham\n",
      "Data extracted for Luks\n",
      "Data extracted for Lynch\n",
      "Data extracted for Thalmann\n",
      "Data extracted for Maibaum\n",
      "Data extracted for Marlow\n",
      "Data extracted for Manna\n",
      "Data extracted for Martin\n",
      "Data extracted for Martin\n",
      "Data extracted for Mashey\n",
      "Data extracted for Matiyasevich\n",
      "Data extracted for Matsumoto\n",
      "Data extracted for Mauchly\n",
      "Data extracted for Maulik\n",
      "Data extracted for McAuley\n",
      "Data extracted for McCarthy\n",
      "Data extracted for McCallum\n",
      "Data extracted for McIlroy\n",
      "Data extracted for McKinstry\n",
      "Data extracted for McKusick\n",
      "Data extracted for Meertens\n",
      "Data extracted for Mehlhorn\n",
      "Data extracted for Metcalf\n",
      "Data extracted for Meyer\n",
      "Data extracted for Micali\n",
      "Data extracted for Milner\n",
      "Data extracted for Minker\n",
      "Data extracted for Minsky\n",
      "Data extracted for Mitchell\n",
      "Data extracted for Mitchell\n",
      "Data extracted for Arvind\n",
      "Data extracted for Mockapetris\n",
      "Data extracted for Moler\n",
      "Data extracted for Moller\n",
      "Data extracted for Moon\n",
      "Data extracted for Moore\n",
      "Data extracted for Moore\n",
      "Data extracted for Moore\n",
      "Data extracted for Moore\n",
      "Data extracted for Moore\n",
      "Data extracted for Moravec\n",
      "Data extracted for Morgan\n",
      "Data extracted for Morris\n",
      "Data extracted for Moses\n",
      "Data extracted for Motwani\n",
      "Data extracted for Mukhanov\n",
      "Data extracted for Muggleton\n",
      "Data extracted for Müller\n",
      "Data extracted for Mycroft\n",
      "Data extracted for Myers\n",
      "Data extracted for Nadin\n",
      "Data extracted for Nagao\n",
      "Data extracted for Nake\n",
      "Data extracted for Nardi\n",
      "Data extracted for Naur\n",
      "Data extracted for Needham\n",
      "Data extracted for Nell\n",
      "Data extracted for Nelson\n",
      "Data extracted for Neumann\n",
      "Data extracted for Neumann\n",
      "Data extracted for Neumann\n",
      "Data extracted for Newell\n",
      "Data extracted for Newman\n",
      "Data extracted for Ng\n",
      "Data extracted for Nilsson\n",
      "Data extracted for Nijssen\n",
      "Data extracted for Nipkow\n",
      "Data extracted for Nivat\n",
      "Data extracted for Nkambule\n",
      "Data extracted for Noe\n",
      "Data extracted for Nordin\n",
      "Data extracted for Norman\n",
      "Data extracted for Norvig\n",
      "Data extracted for Novacky\n",
      "Data extracted for Nygaard\n",
      "Data extracted for Odersky\n",
      "Data extracted for O'Hearn\n",
      "Data extracted for Olle\n",
      "Data extracted for Omohundro\n",
      "Data extracted for Ornstein\n",
      "Data extracted for O'Sullivan\n",
      "Data extracted for Ousterhout\n",
      "Data extracted for Overmars\n",
      "Data extracted for Owicki\n",
      "Data extracted for Page\n",
      "Data extracted for Pal\n",
      "Data extracted for Pandya\n",
      "Data extracted for Papadimitriou\n",
      "Data extracted for Park\n",
      "Data extracted for Parnas\n",
      "Data extracted for Patil\n",
      "Data extracted for Patt\n",
      "Data extracted for Patterson\n",
      "Data extracted for Paterson\n",
      "Data extracted for Pătrașcu\n",
      "Data extracted for Paulson\n",
      "Data extracted for Pausch\n",
      "Data extracted for Pavón\n",
      "Data extracted for Pearl\n",
      "Data extracted for Perlis\n",
      "Data extracted for Perlman\n",
      "Data extracted for Perotto\n",
      "Data extracted for Péter\n",
      "Data extracted for Jones\n",
      "Data extracted for Pham\n",
      "Data extracted for Pieraccini\n",
      "Data extracted for Pingali\n",
      "Data extracted for Plotkin\n",
      "Data extracted for Pnueli\n",
      "Data extracted for Poel\n",
      "Data extracted for Popplewell\n",
      "Data extracted for Post\n",
      "Data extracted for Postel\n",
      "Data extracted for Preparata\n",
      "Data extracted for Press\n",
      "Data extracted for Rabana\n",
      "Data extracted for Rozenberg\n",
      "Data extracted for Rabin\n",
      "Data extracted for Radev\n",
      "Data extracted for Raman\n",
      "Data extracted for Randell\n",
      "Data extracted for Ravn\n",
      "Data extracted for Reddy\n",
      "Data extracted for Reed\n",
      "Data extracted for Reenskaug\n",
      "Data extracted for Reynolds\n",
      "Data extracted for Reynolds\n",
      "Data extracted for Riet\n",
      "Data extracted for Richards\n",
      "Data extracted for Richards\n",
      "Data extracted for Ries\n",
      "Data extracted for Rijsbergen\n",
      "Data extracted for Ritchie\n",
      "Data extracted for Rivest\n",
      "Data extracted for Robinson\n",
      "Data extracted for Rolland\n",
      "Data extracted for Romero\n",
      "Data extracted for Rosenfeld\n",
      "Data extracted for Ross\n",
      "Data extracted for Rossum\n",
      "Data extracted for Rothman\n",
      "Data extracted for Royce\n",
      "Data extracted for Rucker\n",
      "Data extracted for Rudich\n",
      "Data extracted for Rulifson\n",
      "Data extracted for Rumbaugh\n",
      "Data extracted for Ružička\n",
      "Data extracted for Sadowsky\n",
      "Data extracted for Sadrzadeh\n",
      "Data extracted for Saif\n",
      "Data extracted for Salton\n",
      "Data extracted for Sammet\n",
      "Data extracted for Sammut\n",
      "Data extracted for Sassenrath\n",
      "Data extracted for Satyanarayanan\n",
      "Data extracted for Savitch\n",
      "Data extracted for Schaeffer\n",
      "Data extracted for Schickard\n",
      "Data extracted for Schmidhuber\n",
      "Data extracted for Schneider\n",
      "Data extracted for Schneier\n",
      "Data extracted for Schneider\n",
      "Data extracted for Schoenebeck\n",
      "Data extracted for Schroeder\n",
      "Data extracted for Schölkopf\n",
      "Data extracted for Scott\n",
      "Data extracted for Scott\n",
      "Data extracted for Sedgewick\n",
      "Data extracted for Sethi\n",
      "Data extracted for Shadbolt\n",
      "Data extracted for Shamir\n",
      "Data extracted for Shannon\n",
      "Data extracted for Shaw\n",
      "Data extracted for Shaw\n",
      "Data extracted for Shenker\n",
      "Data extracted for Shekhar\n",
      "Data extracted for Shneiderman\n",
      "Data extracted for Shortliffe\n",
      "Data extracted for Siewiorek\n",
      "Data extracted for Sifakis\n",
      "Data extracted for Simon\n",
      "Data extracted for Singh\n",
      "Data extracted for Sitaraman\n",
      "Data extracted for Sleator\n",
      "Data extracted for Sloman\n",
      "Data extracted for Sølvberg\n",
      "Data extracted for Smith\n",
      "Data extracted for Smith\n",
      "Data extracted for Spewak\n",
      "Data extracted for Spradling\n",
      "Data extracted for Sproull\n",
      "Data extracted for Srihari\n",
      "Data extracted for Srihari\n",
      "Data extracted for Stachowiak\n",
      "Data extracted for Stallman\n",
      "Data extracted for Stamper\n",
      "Failed to extract data for URL https://en.wikipedia.org/wiki/Thad_Starner: 'NoneType' object has no attribute 'find_next_sibling'\n",
      "Data extracted for Stearns\n",
      "Data extracted for Jr.\n",
      "Data extracted for Sterling\n",
      "Data extracted for Stepanov\n",
      "Data extracted for Stevens\n",
      "Data extracted for Stockmeyer\n",
      "Data extracted for Stolfo\n",
      "Data extracted for Stonebraker\n",
      "Data extracted for Storaasli\n",
      "Data extracted for Strachey\n",
      "Data extracted for Strassen\n",
      "Data extracted for Stroustrup\n",
      "Data extracted for Sudan\n",
      "Data extracted for Sussman\n",
      "Data extracted for Sutherland\n",
      "Data extracted for Sutherland\n",
      "Data extracted for Sweeney\n",
      "Data extracted for Szegedy\n",
      "Data extracted for Tabriz\n",
      "Data extracted for Tamassia\n",
      "Data extracted for Tanenbaum\n",
      "Data extracted for Tate\n",
      "Data extracted for Thalheim\n",
      "Data extracted for Tardos\n",
      "Data extracted for Tardos\n",
      "Data extracted for Tarjan\n",
      "Data extracted for Taylor\n",
      "Data extracted for Tchou\n",
      "Data extracted for Teevan\n",
      "Data extracted for Teng\n",
      "Data extracted for Tesler\n",
      "Data extracted for Tevanian\n",
      "Data extracted for Thacker\n",
      "Data extracted for Thalmann\n",
      "Data extracted for Thompson\n",
      "Data extracted for Thrun\n",
      "Data extracted for Tichy\n",
      "Data extracted for Toda\n",
      "Data extracted for Torvalds\n",
      "Data extracted for Quevedo\n",
      "Data extracted for Toussaint\n",
      "Data extracted for Townsend\n",
      "Data extracted for Tozer\n",
      "Data extracted for Traub\n",
      "Data extracted for Tucker\n",
      "Data extracted for Tukey\n",
      "Data extracted for Turing\n",
      "Data extracted for Turner\n",
      "Data extracted for Turoff\n",
      "Data extracted for Ullman\n",
      "Data extracted for Valiant\n",
      "Data extracted for Vapnik\n",
      "Data extracted for Vardi\n",
      "Data extracted for Vaughan\n",
      "Data extracted for Vauquois\n",
      "Data extracted for Vazirani\n",
      "Data extracted for Veloso\n",
      "Data extracted for Vernadat\n",
      "Data extracted for Veryard\n",
      "Data extracted for Vilkomir\n",
      "Data extracted for Vitányi\n",
      "Data extracted for Viterbi\n",
      "Data extracted for Vitter\n",
      "Data extracted for Vixie\n",
      "Data extracted for Wada\n",
      "Data extracted for Wagner\n",
      "Data extracted for Waltz\n",
      "Data extracted for Wang\n",
      "Data extracted for Ward\n",
      "Data extracted for Warmuth\n",
      "Data extracted for Warren\n",
      "Data extracted for Warwick\n",
      "Data extracted for Węglarz\n",
      "Data extracted for Wadler\n",
      "Data extracted for Wegner\n",
      "Data extracted for Wegstein\n",
      "Data extracted for Weinberger\n",
      "Data extracted for Weiser\n",
      "Data extracted for Weizenbaum\n",
      "Data extracted for Wheeler\n",
      "Data extracted for Westervelt\n",
      "Data extracted for Whittaker\n",
      "Data extracted for Widom\n",
      "Data extracted for Wiederhold\n",
      "Data extracted for Wiener\n",
      "Data extracted for Wijngaarden\n",
      "Data extracted for Wilkes\n",
      "Data extracted for Wilkes\n",
      "Data extracted for Wilks\n",
      "Data extracted for Wilkinson\n",
      "Data extracted for Wilson\n",
      "Data extracted for Winograd\n",
      "Data extracted for Winograd\n",
      "Data extracted for Winston\n",
      "Data extracted for Wirth\n",
      "Data extracted for Wiseman\n",
      "Data extracted for Wisnosky\n",
      "Data extracted for Wolfram\n",
      "Data extracted for Woodger\n",
      "Data extracted for Woodward\n",
      "Data extracted for Worsley\n",
      "Data extracted for Wozniak\n",
      "Data extracted for Wu\n",
      "Data extracted for Wulf\n",
      "Data extracted for Yannakakis\n",
      "Data extracted for Yao\n",
      "Data extracted for Yen\n",
      "Data extracted for Yoneda\n",
      "Data extracted for Yourdon\n",
      "Data extracted for Yung\n",
      "Data extracted for Zadeh\n",
      "Data extracted for Zantema\n",
      "Data extracted for Zaman\n",
      "Data extracted for Zdonik\n",
      "Data extracted for Zedan\n",
      "Data extracted for Zilberstein\n",
      "Data extracted for Zimmerman\n",
      "Data extracted for Zuse\n",
      "Data extracted for Wikipedia:Contents/Lists\n",
      "Failed to extract data for URL https://en.wikipedia.orghttp://citeseerx.ist.psu.edu/stats/authors?all=true: HTTPSConnectionPool(host='en.wikipedia.orghttp', port=443): Max retries exceeded with url: //citeseerx.ist.psu.edu/stats/authors?all=true (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000024D3BDA28F0>: Failed to resolve 'en.wikipedia.orghttp' ([Errno 11001] getaddrinfo failed)\"))\n",
      "Failed to extract data for URL https://en.wikipedia.orghttp://www.cs.ucla.edu/~palsberg/h-number.html: HTTPSConnectionPool(host='en.wikipedia.orghttp', port=443): Max retries exceeded with url: //www.cs.ucla.edu/~palsberg/h-number.html (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000024D3B942170>: Failed to resolve 'en.wikipedia.orghttp' ([Errno 11001] getaddrinfo failed)\"))\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "# Load the spacy model for text vectorization\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# The main list of computer scientists' Wikipedia URL\n",
    "main_page_url = 'https://en.wikipedia.org/wiki/List_of_computer_scientists'\n",
    "\n",
    "# Function to get the list of scientists' Wikipedia page URLs\n",
    "def get_scientists_urls(main_page_url):\n",
    "    response = requests.get(main_page_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    list_items = soup.select('.mw-parser-output > ul > li')\n",
    "    base_url = 'https://en.wikipedia.org'\n",
    "    scientist_urls = [base_url + item.find('a')['href'] for item in list_items if item.find('a')]\n",
    "    return scientist_urls\n",
    "\n",
    "# Function to extract data from individual Wikipedia page\n",
    "def extract_data_from_page(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Extract the name considering the presence of parenthesis\n",
    "    title = soup.find('h1', {'id': 'firstHeading'}).text\n",
    "    # Check if the title contains parentheses and extract accordingly\n",
    "    if '(' in title:\n",
    "        name = title.split('(')[0].strip().split(' ')[-1]  # Get last word before parenthesis\n",
    "    else:\n",
    "        name = title.split(' ')[-1]  # Get last word of the title if no parenthesis\n",
    "\n",
    "\n",
    "    import re\n",
    "    \n",
    "    # Function to count awards in a given container\n",
    "    def count_awards_in_container(container):\n",
    "        # Find all 'li' tags, if any\n",
    "        list_items = container.find_all('li')\n",
    "        if list_items:\n",
    "            return len(list_items)\n",
    "        else:\n",
    "            # If there are no 'li' tags, it's likely just plain text.\n",
    "            # Here we count the number of awards by splitting the text at <br> tags.\n",
    "            # This is a simple heuristic and might not be accurate if the format is inconsistent.\n",
    "            br_tags = container.find_all('br')\n",
    "            if br_tags:\n",
    "                return len(br_tags) + 1\n",
    "            else:\n",
    "                # If there are no 'br' tags, count each non-empty line as an award\n",
    "                text_awards = container.get_text(separator='\\n').split('\\n')\n",
    "                return len([award for award in text_awards if award.strip() != ''])\n",
    "    \n",
    "    # Start by checking the infobox\n",
    "    infobox = soup.find('table', {'class': 'infobox'})\n",
    "    awards_count = 0\n",
    "    if infobox:\n",
    "        awards_row = infobox.find('th', string=re.compile('Awards', re.I))\n",
    "        if awards_row:\n",
    "            awards_data = awards_row.find_next_sibling('td')\n",
    "            if awards_data:\n",
    "                awards_count = count_awards_in_container(awards_data)\n",
    "    \n",
    "    # If no awards are found in the infobox, look in the main content\n",
    "    if awards_count == 0:\n",
    "        awards_section = soup.find('span', {'class': 'mw-headline'}, string=re.compile('Awards', re.I))\n",
    "        if awards_section:\n",
    "            # Get the container of the awards section which might be within a div or the next 'ul' or 'ol'\n",
    "            next_element = awards_section.find_next()\n",
    "            while next_element and next_element.name not in [\"ul\", \"ol\"]:\n",
    "                next_element = next_element.find_next()\n",
    "            \n",
    "            if next_element and next_element.name in [\"ul\", \"ol\"]:\n",
    "                awards_count = count_awards_in_container(next_element)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Find the 'Alma mater' row in the infobox\n",
    "    infobox = soup.find('table', {'class': 'infobox'})\n",
    "    education_vector = []  # Initialize an empty list to store 'Alma mater' names\n",
    "    if infobox:\n",
    "        # Find 'th' elements with 'infobox-label', then iterate to match 'Alma mater' with non-breaking spaces\n",
    "        for th in infobox.find_all('th', {'class': 'infobox-label'}):\n",
    "            # Use .get_text() and replace to handle non-breaking spaces and compare\n",
    "            if 'Alma mater' in th.get_text().replace(u'\\xa0', u' '):\n",
    "                # If found, get the next sibling 'td' element containing the universities\n",
    "                alma_mater_data = th.find_next_sibling('td')\n",
    "                if alma_mater_data:\n",
    "                    # Get all anchor tags within the 'Alma mater' data cell\n",
    "                    alma_mater_links = alma_mater_data.find_all('a')\n",
    "                    # Extract the text from each anchor tag and add it to the education_vector list\n",
    "                    education_vector = [link.get_text() for link in alma_mater_links if link.get_text().strip()]\n",
    "                break  # Stop the loop after finding the 'Alma mater' row\n",
    "    if not education_vector and infobox:\n",
    "          for th in infobox.find_all('th', {'class': 'infobox-label'}):\n",
    "            # Use .get_text() and replace to handle non-breaking spaces and compare\n",
    "            if 'Education' in th.get_text().replace(u'\\xa0', u' '):\n",
    "                # If found, get the next sibling 'td' element containing the universities\n",
    "                alma_mater_data = th.find_next_sibling('td')\n",
    "                if alma_mater_data:\n",
    "                    # Get all anchor tags within the 'Alma mater' data cell\n",
    "                    alma_mater_links = alma_mater_data.find_all('a')\n",
    "                    # Extract the text from each anchor tag and add it to the education_vector list\n",
    "                    education_vector = [link.get_text() for link in alma_mater_links if link.get_text().strip()]\n",
    "                break  # Stop the loop after finding the 'Alma mater' row\n",
    "    if not education_vector:\n",
    "        education_section = soup.find('span', {'class': 'mw-headline', 'id': 'Education'})\n",
    "        if education_section:\n",
    "            # Get the container of the education section which is usually a preceding sibling of h2 containing the 'Education' span\n",
    "            edu_container = education_section.find_parent('h2').find_next_sibling(lambda tag: tag.name in [\"ul\", \"p\", \"div\"])\n",
    "            if edu_container:\n",
    "                university_links = edu_container.find_all('a', string=lambda text: 'Uni' in text if text else False)\n",
    "                if university_links:\n",
    "                    # Add the text of the first valid 'University' link\n",
    "                    education_vector.append(university_links[0].get_text())\n",
    "    \n",
    "    if not education_vector:\n",
    "        bio_section = soup.find('span', {'class': 'mw-headline', 'id': 'Biography'})\n",
    "        if bio_section:\n",
    "            # Get the container of the education section which is usually a preceding sibling of h2 containing the 'Education' span\n",
    "            bio_container = bio_section.find_parent('h2').find_next_sibling(lambda tag: tag.name in [\"ul\", \"p\", \"div\"])\n",
    "            if bio_container:\n",
    "                university_links = bio_container.find_all('a', string=lambda text: 'Uni' in text if text else False)\n",
    "                if university_links:\n",
    "                    # Add the text of the first valid link containing 'Uni'\n",
    "                    education_vector.append(university_links[0].get_text())\n",
    "    \n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    return {\n",
    "        'Surname': name,\n",
    "        '#Awards': awards_count,\n",
    "        'Education': education_vector  # Convert numpy array to list for easier handling\n",
    "    }\n",
    "\n",
    "# Get the list of individual Wikipedia URLs for the scientists\n",
    "scientists_urls = get_scientists_urls(main_page_url)\n",
    "\n",
    "# List to hold the data\n",
    "data = []\n",
    "\n",
    "# Iterate over the URLs and extract data\n",
    "for url in scientists_urls[:]:  # Limiting to first 10 for demonstration\n",
    "    try:\n",
    "        scientist_data = extract_data_from_page(url)\n",
    "        data.append(scientist_data)\n",
    "        print(f\"Data extracted for {scientist_data['Surname']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to extract data for URL {url}: {e}\")\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Optionally, save the DataFrame to a CSV file\n",
    "df.to_csv('computer_scientists_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08c8e8fd-df5f-41e8-bdde-5f770f485a63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Surname</th>\n",
       "      <th>#Awards</th>\n",
       "      <th>Education</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Khan</td>\n",
       "      <td>10</td>\n",
       "      <td>[University of Malaya, COMSATS University]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aalst</td>\n",
       "      <td>0</td>\n",
       "      <td>[Technische Universiteit Eindhoven]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aaronson</td>\n",
       "      <td>4</td>\n",
       "      <td>[Cornell University, University of California,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abebe</td>\n",
       "      <td>3</td>\n",
       "      <td>[Cornell University, University of Cambridge, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Abelson</td>\n",
       "      <td>1</td>\n",
       "      <td>[Princeton University, Massachusetts Institute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Abiteboul</td>\n",
       "      <td>4</td>\n",
       "      <td>[University of Southern California]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Abramsky</td>\n",
       "      <td>4</td>\n",
       "      <td>[King's College, Cambridge, Queen Mary Univers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Adleman</td>\n",
       "      <td>2</td>\n",
       "      <td>[University of California, Berkeley]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Agrawal</td>\n",
       "      <td>8</td>\n",
       "      <td>[Indian Institute of Technology Kanpur]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ahn</td>\n",
       "      <td>3</td>\n",
       "      <td>[Duke University, BS, Carnegie Mellon University]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Aho</td>\n",
       "      <td>7</td>\n",
       "      <td>[University of Toronto, Princeton University]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Allen</td>\n",
       "      <td>6</td>\n",
       "      <td>[University at Albany, University of Michigan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Amdahl</td>\n",
       "      <td>2</td>\n",
       "      <td>[South Dakota State University, BS, University...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Anderson</td>\n",
       "      <td>2</td>\n",
       "      <td>[Wesleyan University, University of Wisconsin-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Anthony</td>\n",
       "      <td>6</td>\n",
       "      <td>[Drexel University]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Appel</td>\n",
       "      <td>0</td>\n",
       "      <td>[Princeton University]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Aragon</td>\n",
       "      <td>1</td>\n",
       "      <td>[California Institute of Technology, Universit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Arden</td>\n",
       "      <td>0</td>\n",
       "      <td>[Purdue University]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Jones</td>\n",
       "      <td>0</td>\n",
       "      <td>[Tennessee State University, North Carolina St...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Arora</td>\n",
       "      <td>0</td>\n",
       "      <td>[SB, Massachusetts Institute of Technology, Ph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Asprey</td>\n",
       "      <td>0</td>\n",
       "      <td>[Vassar College, University of Iowa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Atanasoff</td>\n",
       "      <td>2</td>\n",
       "      <td>[University of Florida, Iowa State University,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Atre</td>\n",
       "      <td>0</td>\n",
       "      <td>[University of Poona, University of Heidelberg]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Babbage</td>\n",
       "      <td>2</td>\n",
       "      <td>[Peterhouse, Cambridge]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Bachman</td>\n",
       "      <td>3</td>\n",
       "      <td>[University of Pennsylvania, Michigan State Un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Backhouse</td>\n",
       "      <td>0</td>\n",
       "      <td>[Churchill College, Cambridge, Imperial Colleg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Backus</td>\n",
       "      <td>3</td>\n",
       "      <td>[University of Virginia, University of Pittsbu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Bacon</td>\n",
       "      <td>1</td>\n",
       "      <td>[U.C. Berkeley]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Bader</td>\n",
       "      <td>5</td>\n",
       "      <td>[Lehigh University, University of Maryland, Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Bahl</td>\n",
       "      <td>4</td>\n",
       "      <td>[University of Massachusetts, Amherst, Modern ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Surname  #Awards                                          Education\n",
       "0        Khan       10         [University of Malaya, COMSATS University]\n",
       "1       Aalst        0                [Technische Universiteit Eindhoven]\n",
       "2    Aaronson        4  [Cornell University, University of California,...\n",
       "3       Abebe        3  [Cornell University, University of Cambridge, ...\n",
       "4     Abelson        1  [Princeton University, Massachusetts Institute...\n",
       "5   Abiteboul        4                [University of Southern California]\n",
       "6    Abramsky        4  [King's College, Cambridge, Queen Mary Univers...\n",
       "7     Adleman        2               [University of California, Berkeley]\n",
       "8     Agrawal        8            [Indian Institute of Technology Kanpur]\n",
       "9         Ahn        3  [Duke University, BS, Carnegie Mellon University]\n",
       "10        Aho        7      [University of Toronto, Princeton University]\n",
       "11      Allen        6     [University at Albany, University of Michigan]\n",
       "12     Amdahl        2  [South Dakota State University, BS, University...\n",
       "13   Anderson        2  [Wesleyan University, University of Wisconsin-...\n",
       "14    Anthony        6                                [Drexel University]\n",
       "15      Appel        0                             [Princeton University]\n",
       "16     Aragon        1  [California Institute of Technology, Universit...\n",
       "17      Arden        0                                [Purdue University]\n",
       "18      Jones        0  [Tennessee State University, North Carolina St...\n",
       "19      Arora        0  [SB, Massachusetts Institute of Technology, Ph...\n",
       "20     Asprey        0               [Vassar College, University of Iowa]\n",
       "21  Atanasoff        2  [University of Florida, Iowa State University,...\n",
       "22       Atre        0    [University of Poona, University of Heidelberg]\n",
       "23    Babbage        2                            [Peterhouse, Cambridge]\n",
       "24    Bachman        3  [University of Pennsylvania, Michigan State Un...\n",
       "25  Backhouse        0  [Churchill College, Cambridge, Imperial Colleg...\n",
       "26     Backus        3  [University of Virginia, University of Pittsbu...\n",
       "27      Bacon        1                                    [U.C. Berkeley]\n",
       "28      Bader        5  [Lehigh University, University of Maryland, Co...\n",
       "29       Bahl        4  [University of Massachusetts, Amherst, Modern ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cab4e49-7251-405f-a2c3-643df4d65788",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
